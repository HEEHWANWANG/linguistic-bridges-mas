"""
Calculate Creativity Scores for Batch-Generated Visual Prompts

Processes JSON files generated by batch prompt generation and calculates:
- Individual prompt creativity scores (originality, elaboration, alignment, coherence)
- Sample-level aggregates
- Dataset-level statistics
- Comparative analysis (convergent vs divergent)
- Feature correlations

Output: Multiple JSON files with detailed scores and analysis
"""

import asyncio
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
import statistics

from creativity_evaluator import (
    MusicToImageCreativityEvaluator,
    CreativityMetrics
)


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('calculate_creativity.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class PromptCreativityScore:
    """Individual prompt creativity score"""
    sample_idx: int
    prompt_id: int
    mode: str  # 'convergent' or 'divergent'
    temperature: float
    originality: float
    elaboration: float
    alignment: float
    coherence: float
    overall: float
    prompt_text: str  # Original prompt for reference

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return asdict(self)


@dataclass
class SampleCreativitySummary:
    """Summary statistics for all prompts in a sample"""
    sample_idx: int
    num_prompts: int
    convergent_count: int
    divergent_count: int

    # Average scores
    avg_originality: float
    avg_elaboration: float
    avg_alignment: float
    avg_coherence: float
    avg_overall: float

    # Convergent scores
    conv_originality: float
    conv_elaboration: float
    conv_alignment: float
    conv_coherence: float
    conv_overall: float

    # Divergent scores
    div_originality: float
    div_elaboration: float
    div_alignment: float
    div_coherence: float
    div_overall: float

    # Features
    musical_features: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        result = asdict(self)
        result['musical_features'] = self.musical_features
        return result


@dataclass
class DatasetCreativityStats:
    """Overall dataset statistics"""
    total_samples: int
    total_prompts: int
    generation_timestamp: str
    evaluation_timestamp: str

    # Overall averages
    mean_originality: float
    mean_elaboration: float
    mean_alignment: float
    mean_coherence: float
    mean_overall: float

    # Standard deviations
    std_originality: float
    std_elaboration: float
    std_alignment: float
    std_coherence: float
    std_overall: float

    # Min/Max
    min_overall: float
    max_overall: float

    # Convergent vs Divergent
    convergent_mean: float
    divergent_mean: float
    convergent_std: float
    divergent_std: float

    # Feature correlations
    correlations: Dict[str, Dict[str, float]]

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return asdict(self)


class BatchCreativityCalculator:
    """Calculate creativity scores for batch-generated prompts"""

    def __init__(
        self,
        prompts_file: str = "generated_prompts/visual_prompts_complete.json",
        output_dir: str = "creativity_scores"
    ):
        """
        Initialize calculator.

        Args:
            prompts_file: Path to visual_prompts_complete.json
            output_dir: Directory to save results
        """
        self.prompts_file = Path(prompts_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Load prompts
        logger.info(f"Loading prompts from {prompts_file}")
        with open(self.prompts_file, 'r') as f:
            self.data = json.load(f)

        self.metadata = self.data.get('metadata', {})
        self.samples = self.data.get('samples', [])

        logger.info(f"Loaded {len(self.samples)} samples with prompts")

        # Initialize evaluator
        self.evaluator = MusicToImageCreativityEvaluator()

        # Storage
        self.prompt_scores: List[PromptCreativityScore] = []
        self.sample_summaries: List[SampleCreativitySummary] = []
        self.dataset_stats: Optional[DatasetCreativityStats] = None

    async def calculate_prompt_scores(self) -> List[PromptCreativityScore]:
        """
        Calculate creativity scores for all prompts.

        Returns:
            List of PromptCreativityScore objects
        """
        logger.info(f"Calculating creativity scores for all prompts...")
        self.prompt_scores = []

        for sample_idx, sample in enumerate(self.samples):
            if sample_idx % 10 == 0:
                logger.info(f"Processing sample {sample_idx + 1}/{len(self.samples)}")

            sample_idx_val = sample.get('sample_idx', sample_idx)
            musical_features = sample.get('musical_features', {})
            prompts = sample.get('prompts', [])

            for prompt_data in prompts:
                visual_prompt = prompt_data.get('visual_prompt', '')

                # Calculate score
                metrics = self.evaluator.evaluate(visual_prompt, musical_features)

                # Create score object
                score = PromptCreativityScore(
                    sample_idx=sample_idx_val,
                    prompt_id=prompt_data.get('prompt_id', 0),
                    mode=prompt_data.get('mode', 'unknown'),
                    temperature=prompt_data.get('temperature', 0.0),
                    originality=metrics.originality,
                    elaboration=metrics.elaboration,
                    alignment=metrics.alignment,
                    coherence=metrics.coherence,
                    overall=metrics.overall,
                    prompt_text=visual_prompt
                )

                self.prompt_scores.append(score)

        logger.info(f"✓ Calculated {len(self.prompt_scores)} prompt scores")
        return self.prompt_scores

    def calculate_sample_summaries(self) -> List[SampleCreativitySummary]:
        """
        Calculate summary statistics for each sample.

        Returns:
            List of SampleCreativitySummary objects
        """
        logger.info("Calculating sample-level summaries...")
        self.sample_summaries = []

        for sample in self.samples:
            sample_idx = sample.get('sample_idx', 0)
            musical_features = sample.get('musical_features', {})

            # Get scores for this sample
            sample_scores = [
                s for s in self.prompt_scores
                if s.sample_idx == sample_idx
            ]

            if not sample_scores:
                continue

            # Separate convergent and divergent
            convergent = [s for s in sample_scores if s.mode == 'convergent']
            divergent = [s for s in sample_scores if s.mode == 'divergent']

            # Calculate averages
            def avg_score(scores: List[PromptCreativityScore], field: str) -> float:
                if not scores:
                    return 0.0
                return sum(getattr(s, field) for s in scores) / len(scores)

            summary = SampleCreativitySummary(
                sample_idx=sample_idx,
                num_prompts=len(sample_scores),
                convergent_count=len(convergent),
                divergent_count=len(divergent),

                # Overall averages
                avg_originality=avg_score(sample_scores, 'originality'),
                avg_elaboration=avg_score(sample_scores, 'elaboration'),
                avg_alignment=avg_score(sample_scores, 'alignment'),
                avg_coherence=avg_score(sample_scores, 'coherence'),
                avg_overall=avg_score(sample_scores, 'overall'),

                # Convergent
                conv_originality=avg_score(convergent, 'originality'),
                conv_elaboration=avg_score(convergent, 'elaboration'),
                conv_alignment=avg_score(convergent, 'alignment'),
                conv_coherence=avg_score(convergent, 'coherence'),
                conv_overall=avg_score(convergent, 'overall'),

                # Divergent
                div_originality=avg_score(divergent, 'originality'),
                div_elaboration=avg_score(divergent, 'elaboration'),
                div_alignment=avg_score(divergent, 'alignment'),
                div_coherence=avg_score(divergent, 'coherence'),
                div_overall=avg_score(divergent, 'overall'),

                # Features
                musical_features=musical_features
            )

            self.sample_summaries.append(summary)

        logger.info(f"✓ Created summaries for {len(self.sample_summaries)} samples")
        return self.sample_summaries

    def calculate_dataset_stats(self) -> DatasetCreativityStats:
        """
        Calculate overall dataset statistics.

        Returns:
            DatasetCreativityStats object
        """
        logger.info("Calculating dataset-level statistics...")

        if not self.prompt_scores:
            raise ValueError("No prompt scores calculated yet")

        # Extract scores
        originalities = [s.originality for s in self.prompt_scores]
        elaborations = [s.elaboration for s in self.prompt_scores]
        alignments = [s.alignment for s in self.prompt_scores]
        coherences = [s.coherence for s in self.prompt_scores]
        overalls = [s.overall for s in self.prompt_scores]

        # Convergent vs Divergent
        convergent_scores = [s.overall for s in self.prompt_scores if s.mode == 'convergent']
        divergent_scores = [s.overall for s in self.prompt_scores if s.mode == 'divergent']

        # Calculate correlations with musical features
        correlations = self._calculate_correlations()

        stats = DatasetCreativityStats(
            total_samples=len(self.samples),
            total_prompts=len(self.prompt_scores),
            generation_timestamp=self.metadata.get('generation_timestamp', 'unknown'),
            evaluation_timestamp=datetime.now().isoformat(),

            # Means
            mean_originality=statistics.mean(originalities),
            mean_elaboration=statistics.mean(elaborations),
            mean_alignment=statistics.mean(alignments),
            mean_coherence=statistics.mean(coherences),
            mean_overall=statistics.mean(overalls),

            # Std devs
            std_originality=statistics.stdev(originalities) if len(originalities) > 1 else 0.0,
            std_elaboration=statistics.stdev(elaborations) if len(elaborations) > 1 else 0.0,
            std_alignment=statistics.stdev(alignments) if len(alignments) > 1 else 0.0,
            std_coherence=statistics.stdev(coherences) if len(coherences) > 1 else 0.0,
            std_overall=statistics.stdev(overalls) if len(overalls) > 1 else 0.0,

            # Min/Max
            min_overall=min(overalls),
            max_overall=max(overalls),

            # Mode comparison
            convergent_mean=statistics.mean(convergent_scores) if convergent_scores else 0.0,
            divergent_mean=statistics.mean(divergent_scores) if divergent_scores else 0.0,
            convergent_std=statistics.stdev(convergent_scores) if len(convergent_scores) > 1 else 0.0,
            divergent_std=statistics.stdev(divergent_scores) if len(divergent_scores) > 1 else 0.0,

            # Correlations
            correlations=correlations
        )

        self.dataset_stats = stats
        logger.info("✓ Dataset statistics calculated")
        return stats

    def _calculate_correlations(self) -> Dict[str, Dict[str, float]]:
        """
        Calculate correlations between musical features and creativity scores.

        Returns:
            Dictionary of correlations
        """
        correlations = {}

        # Collect feature-score pairs
        feature_keys = ['tempo', 'tonality', 'mood']
        metrics_keys = ['originality', 'elaboration', 'alignment', 'coherence', 'overall']

        for feature_key in feature_keys:
            correlations[feature_key] = {}

            for metric_key in metrics_keys:
                # Collect pairs
                pairs = []
                for summary in self.sample_summaries:
                    features = summary.musical_features
                    feature_value = features.get(feature_key)

                    if feature_value is None:
                        continue

                    # Convert feature to numeric if needed
                    if feature_key == 'tempo':
                        try:
                            feature_num = float(feature_value)
                        except (ValueError, TypeError):
                            continue
                    else:
                        # For categorical, skip for now
                        continue

                    metric_value = getattr(summary, f'avg_{metric_key}')
                    pairs.append((feature_num, metric_value))

                # Calculate correlation if enough pairs
                if len(pairs) < 2:
                    correlations[feature_key][metric_key] = 0.0
                    continue

                try:
                    # Pearson correlation
                    x_vals = [p[0] for p in pairs]
                    y_vals = [p[1] for p in pairs]

                    x_mean = statistics.mean(x_vals)
                    y_mean = statistics.mean(y_vals)

                    numerator = sum(
                        (x_vals[i] - x_mean) * (y_vals[i] - y_mean)
                        for i in range(len(x_vals))
                    )
                    x_std = statistics.stdev(x_vals) if len(x_vals) > 1 else 1.0
                    y_std = statistics.stdev(y_vals) if len(y_vals) > 1 else 1.0

                    correlation = numerator / (x_std * y_std * len(x_vals))
                    correlations[feature_key][metric_key] = correlation
                except Exception as e:
                    logger.warning(f"Could not calculate correlation for {feature_key}/{metric_key}: {e}")
                    correlations[feature_key][metric_key] = 0.0

        return correlations

    async def run_full_evaluation(self) -> Tuple[
        List[PromptCreativityScore],
        List[SampleCreativitySummary],
        DatasetCreativityStats
    ]:
        """
        Run complete evaluation pipeline.

        Returns:
            Tuple of (prompt scores, sample summaries, dataset stats)
        """
        logger.info("Starting complete evaluation pipeline...")

        # Calculate all scores
        await self.calculate_prompt_scores()

        # Calculate summaries
        self.calculate_sample_summaries()

        # Calculate stats
        self.calculate_dataset_stats()

        logger.info("✓ Evaluation pipeline complete")

        return self.prompt_scores, self.sample_summaries, self.dataset_stats

    def save_prompt_scores(self, filename: str = "creativity_prompt_scores.json") -> Path:
        """Save individual prompt scores."""
        filepath = self.output_dir / filename
        data = [s.to_dict() for s in self.prompt_scores]

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)

        logger.info(f"Saved prompt scores to {filepath}")
        return filepath

    def save_sample_summaries(self, filename: str = "creativity_sample_summaries.json") -> Path:
        """Save sample-level summaries."""
        filepath = self.output_dir / filename
        data = [s.to_dict() for s in self.sample_summaries]

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)

        logger.info(f"Saved sample summaries to {filepath}")
        return filepath

    def save_dataset_stats(self, filename: str = "creativity_dataset_stats.json") -> Path:
        """Save dataset statistics."""
        filepath = self.output_dir / filename

        with open(filepath, 'w') as f:
            json.dump(self.dataset_stats.to_dict(), f, indent=2)

        logger.info(f"Saved dataset stats to {filepath}")
        return filepath

    def save_comparison_report(self, filename: str = "creativity_comparison_report.json") -> Path:
        """Save convergent vs divergent comparison."""
        filepath = self.output_dir / filename

        report = {
            "convergent_vs_divergent": {
                "convergent_mean": self.dataset_stats.convergent_mean,
                "convergent_std": self.dataset_stats.convergent_std,
                "divergent_mean": self.dataset_stats.divergent_mean,
                "divergent_std": self.dataset_stats.divergent_std,
                "difference": self.dataset_stats.divergent_mean - self.dataset_stats.convergent_mean,
                "convergent_count": sum(1 for s in self.prompt_scores if s.mode == 'convergent'),
                "divergent_count": sum(1 for s in self.prompt_scores if s.mode == 'divergent')
            },
            "by_dimension": {
                "originality": {
                    "convergent": statistics.mean([s.originality for s in self.prompt_scores if s.mode == 'convergent']),
                    "divergent": statistics.mean([s.originality for s in self.prompt_scores if s.mode == 'divergent'])
                },
                "elaboration": {
                    "convergent": statistics.mean([s.elaboration for s in self.prompt_scores if s.mode == 'convergent']),
                    "divergent": statistics.mean([s.elaboration for s in self.prompt_scores if s.mode == 'divergent'])
                },
                "alignment": {
                    "convergent": statistics.mean([s.alignment for s in self.prompt_scores if s.mode == 'convergent']),
                    "divergent": statistics.mean([s.alignment for s in self.prompt_scores if s.mode == 'divergent'])
                },
                "coherence": {
                    "convergent": statistics.mean([s.coherence for s in self.prompt_scores if s.mode == 'convergent']),
                    "divergent": statistics.mean([s.coherence for s in self.prompt_scores if s.mode == 'divergent'])
                }
            }
        }

        with open(filepath, 'w') as f:
            json.dump(report, f, indent=2)

        logger.info(f"Saved comparison report to {filepath}")
        return filepath

    def print_summary(self):
        """Print summary of creativity evaluation."""
        if not self.dataset_stats:
            print("No statistics calculated yet")
            return

        stats = self.dataset_stats

        print("\n" + "=" * 70)
        print("CREATIVITY EVALUATION RESULTS")
        print("=" * 70)

        print(f"\nDataset Overview:")
        print(f"  Total samples: {stats.total_samples}")
        print(f"  Total prompts: {stats.total_prompts}")
        print(f"  Generation time: {stats.generation_timestamp}")
        print(f"  Evaluation time: {stats.evaluation_timestamp}")

        print(f"\nOverall Creativity Scores (1-5):")
        print(f"  Mean:     {stats.mean_overall:.2f} ± {stats.std_overall:.2f}")
        print(f"  Range:    {stats.min_overall:.2f} - {stats.max_overall:.2f}")

        print(f"\nBy Dimension (1-5):")
        print(f"  Originality:  {stats.mean_originality:.2f} ± {stats.std_originality:.2f}")
        print(f"  Elaboration:  {stats.mean_elaboration:.2f} ± {stats.std_elaboration:.2f}")
        print(f"  Alignment:    {stats.mean_alignment:.2f} ± {stats.std_alignment:.2f}")
        print(f"  Coherence:    {stats.mean_coherence:.2f} ± {stats.std_coherence:.2f}")

        print(f"\nConvergent vs Divergent:")
        print(f"  Convergent (T=0.4): {stats.convergent_mean:.2f} ± {stats.convergent_std:.2f}")
        print(f"  Divergent (T=0.8):  {stats.divergent_mean:.2f} ± {stats.divergent_std:.2f}")
        print(f"  Difference:         {stats.divergent_mean - stats.convergent_mean:+.2f}")

        print(f"\nFeature Correlations (with overall creativity):")
        if 'tempo' in stats.correlations:
            print(f"  Tempo-Creativity:   {stats.correlations['tempo'].get('overall', 0.0):.3f}")

        print(f"\nOutput Files:")
        print(f"  Directory: {self.output_dir.absolute()}")
        print(f"  - creativity_prompt_scores.json (all individual scores)")
        print(f"  - creativity_sample_summaries.json (per-sample summaries)")
        print(f"  - creativity_dataset_stats.json (overall statistics)")
        print(f"  - creativity_comparison_report.json (convergent vs divergent)")

        print("=" * 70 + "\n")


async def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Calculate creativity scores for batch prompts")
    parser.add_argument(
        "--prompts-file",
        default="generated_prompts/visual_prompts_complete.json",
        help="Path to prompts JSON file"
    )
    parser.add_argument(
        "--output-dir",
        default="creativity_scores",
        help="Output directory for results"
    )

    args = parser.parse_args()

    # Create calculator
    calculator = BatchCreativityCalculator(
        prompts_file=args.prompts_file,
        output_dir=args.output_dir
    )

    # Run evaluation
    try:
        await calculator.run_full_evaluation()

        # Save results
        calculator.save_prompt_scores()
        calculator.save_sample_summaries()
        calculator.save_dataset_stats()
        calculator.save_comparison_report()

        # Print summary
        calculator.print_summary()

    except Exception as e:
        logger.error(f"Error during evaluation: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
